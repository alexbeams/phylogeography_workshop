[
  {
    "objectID": "topics/sse_models/fit_bisse.html",
    "href": "topics/sse_models/fit_bisse.html",
    "title": "Fitting BiSSE models:",
    "section": "",
    "text": "library(diversitree)\nlibrary(mcmcensemble)\n\nNow that we have some familiarity with the behavior of the BISSE model, we will try to use the model for parameter estimation, i.e. inference via model fitting.\nOur goals are:\n\nLearn how to specify a BiSSE model log-likelihood\nLearn how to fit a BiSSE model\nLearn how to constrain models to simplify models/estimate fewer parameters\nDevelop an intuition for how well we can estimate parameters in practice\n\n\n\nSimulate a BiSSE tree:\n\npars &lt;- c(\n        lambda0=1,\n        lambda1=5,\n        mu0=0.8,\n        mu1=0.8,\n        q01=0.1,\n        q10=0.12)\n\ntree &lt;- tree.bisse(pars, max.taxa=50, x0=0)\n\nThe make.bisse function is a scalar function of two variables of the form loglik = loglik(c(lambda,mu))\n\nloglik &lt;- make.bisse(tree, tree$tip.state)\n\nWhat are the MLEs?\n\nmles &lt;- find.mle(loglik, pars)\n\n# these are the parmeter values that this infers: \nmlepars &lt;- mles$par\n\nLet’s visualize the likelihood surface one parameter at a time (holding the other parameter at the MLE; not exactly profile likelihoods, but pretty close):\n\nloglik_lambda0 &lt;- function(lambda0) loglik(c(lambda0,mlepars[-1]))\nloglik_lambda1 &lt;- function(lambda1) loglik(c(mlepars[1],lambda1,mlepars[3:6]))\n\nloglik_mu0 &lt;- function(mu0) loglik(c(mlepars[1:2],mu0,mlepars[4:6]))\nloglik_mu1 &lt;- function(mu1) loglik(c(mlepars[1:3],mu1,mlepars[5:6]))\n\nloglik_q01 &lt;- function(q01) loglik(c(mlepars[1:4],q01,mlepars[6]))\nloglik_q10 &lt;- function(q10) loglik(c(mlepars[1:5],q10))\n\nlambdavals &lt;- exp( seq(-5,2,length=50))\nmuvals &lt;- exp( seq(-9,2,length=50))\nqvals &lt;- exp( seq(-9,2, length=50) )\n\nloglik_lambda0_vals &lt;- sapply(lambdavals, loglik_lambda0)\nloglik_lambda1_vals &lt;- sapply(lambdavals, loglik_lambda1)\n\nloglik_mu0_vals &lt;- sapply(muvals, loglik_mu0)\nloglik_mu1_vals &lt;- sapply(muvals, loglik_mu1)\n\nloglik_q01_vals &lt;- sapply(qvals, loglik_q01)\nloglik_q10_vals &lt;- sapply(qvals, loglik_q10)\n\npar(mfrow=c(3,2))\n\nplot(lambdavals,loglik_lambda0_vals)\nabline(v=pars['lambda0'],col='red')\n\nplot(lambdavals,loglik_lambda1_vals)\nabline(v=pars['lambda1'],col='red')\n\nplot(muvals,loglik_mu0_vals)\nabline(v=pars['mu0'],col='red')\n\nplot(muvals,loglik_mu1_vals)\nabline(v=pars['mu1'],col='red')\n\nplot(qvals,loglik_q01_vals)\nabline(v=pars['q01'],col='red')\nplot(qvals,loglik_q10_vals)\nabline(v=pars['q10'],col='red')\n\nSo, varying each parameter while holding the others at the values used to produce the simulated BiSSE tree indicate MLEs may be a good approach to estimating parameters.\nThe MLE differs from the “true” values used to produce the simulation, but that is expected.\nOf course, these approximations of profile likelihoods don’t show how correlated some of these parameters are with each other. We might as well use MCMC to do this:\nThis will take quite a lot longer than our birth-death model with just 2 parameters:\n\nmcmc_fit &lt;- mcmc(loglik, pars, nsteps = 1000, w=1) \n\nLet’s use an ensemble MCMC sampler.\nThe make.bisse function will throw errors if we try to plug in negative parameters:\n\nloglik(-pars)\n\nFor an MCMC routine that isn’t included in diversitree, we need to ensure that we can do an unrestricted search in parameter space.\n\nloglik_transformed &lt;- function(pars){\n    pars &lt;- exp(pars)\n    loglik &lt;- loglik(pars)\n    return(loglik)\n}\n\nloglik_transformed(log(pars))\n# This looks ok.\n\nloglik_transformed(-log(pars))\n# Calculates a value without error. Good to go.\n\nnwalkers &lt;- 100 #ensemble size\nnsteps &lt;- 50 #number of times to update entire nsemble\n\n# initialize the ensemble:\nmcmc_inits &lt;- matrix(nrow=nwalkers, ncol=6)\nfor(i in 1:nwalkers) mcmc_inits[i,] &lt;- jitter(log(mlepars))\n\n# run the ensemble MCMC:\nmcmc_fit &lt;- MCMCEnsemble(loglik_transformed, \n    inits=mcmc_inits, \n    max.iter=nwalkers*nsteps, \n    n.walkers = nwalkers)\n\n# Now, let's try to visualize the profile log-likelihoods (including\n#   the curves we generated earlier):\n\npar(mfrow=c(3,2))\n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,1],xlab=bquote(lambda[0]))\n#lines(log(lambdavals), loglik_lambda0_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,2],xlab=bquote(lambda[1]))\n#lines(log(lambdavals), loglik_lambda1_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,3],xlab=bquote(mu[0]))\n#lines(log(muvals), loglik_mu0_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,4],xlab=bquote(mu[1]))\n#lines(log(muvals), loglik_mu1_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,5],xlab=bquote(q[0~1]))\n#lines(log(qvals), loglik_q01_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,6],xlab=bquote(q[1~0]))\n#lines(log(qvals), loglik_q10_vals, col='red') \n\nIn the vicinity of the MLE, the MCMC results should closely match the profile likelihood curves (but far away from the MLE there is no reason why they should be similar). (Why?)\nLooking at the different plots, are you able to say which parameters that are more difficult to estimate than others?\nWhat if we look at parameter correlations?\n\nplot(mcmc_fit$samples[,,1] ~ mcmc_fit$samples[,,2],\n    xlab=bquote(lambda[1]),\n    ylab=bquote(lambda[0]))"
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#example-benchmarking-from-simulations",
    "href": "topics/sse_models/fit_bisse.html#example-benchmarking-from-simulations",
    "title": "Fitting BiSSE models:",
    "section": "",
    "text": "Simulate a BiSSE tree:\n\npars &lt;- c(\n        lambda0=1,\n        lambda1=5,\n        mu0=0.8,\n        mu1=0.8,\n        q01=0.1,\n        q10=0.12)\n\ntree &lt;- tree.bisse(pars, max.taxa=50, x0=0)\n\nThe make.bisse function is a scalar function of two variables of the form loglik = loglik(c(lambda,mu))\n\nloglik &lt;- make.bisse(tree, tree$tip.state)\n\nWhat are the MLEs?\n\nmles &lt;- find.mle(loglik, pars)\n\n# these are the parmeter values that this infers: \nmlepars &lt;- mles$par\n\nLet’s visualize the likelihood surface one parameter at a time (holding the other parameter at the MLE; not exactly profile likelihoods, but pretty close):\n\nloglik_lambda0 &lt;- function(lambda0) loglik(c(lambda0,mlepars[-1]))\nloglik_lambda1 &lt;- function(lambda1) loglik(c(mlepars[1],lambda1,mlepars[3:6]))\n\nloglik_mu0 &lt;- function(mu0) loglik(c(mlepars[1:2],mu0,mlepars[4:6]))\nloglik_mu1 &lt;- function(mu1) loglik(c(mlepars[1:3],mu1,mlepars[5:6]))\n\nloglik_q01 &lt;- function(q01) loglik(c(mlepars[1:4],q01,mlepars[6]))\nloglik_q10 &lt;- function(q10) loglik(c(mlepars[1:5],q10))\n\nlambdavals &lt;- exp( seq(-5,2,length=50))\nmuvals &lt;- exp( seq(-9,2,length=50))\nqvals &lt;- exp( seq(-9,2, length=50) )\n\nloglik_lambda0_vals &lt;- sapply(lambdavals, loglik_lambda0)\nloglik_lambda1_vals &lt;- sapply(lambdavals, loglik_lambda1)\n\nloglik_mu0_vals &lt;- sapply(muvals, loglik_mu0)\nloglik_mu1_vals &lt;- sapply(muvals, loglik_mu1)\n\nloglik_q01_vals &lt;- sapply(qvals, loglik_q01)\nloglik_q10_vals &lt;- sapply(qvals, loglik_q10)\n\npar(mfrow=c(3,2))\n\nplot(lambdavals,loglik_lambda0_vals)\nabline(v=pars['lambda0'],col='red')\n\nplot(lambdavals,loglik_lambda1_vals)\nabline(v=pars['lambda1'],col='red')\n\nplot(muvals,loglik_mu0_vals)\nabline(v=pars['mu0'],col='red')\n\nplot(muvals,loglik_mu1_vals)\nabline(v=pars['mu1'],col='red')\n\nplot(qvals,loglik_q01_vals)\nabline(v=pars['q01'],col='red')\nplot(qvals,loglik_q10_vals)\nabline(v=pars['q10'],col='red')\n\nSo, varying each parameter while holding the others at the values used to produce the simulated BiSSE tree indicate MLEs may be a good approach to estimating parameters.\nThe MLE differs from the “true” values used to produce the simulation, but that is expected.\nOf course, these approximations of profile likelihoods don’t show how correlated some of these parameters are with each other. We might as well use MCMC to do this:\nThis will take quite a lot longer than our birth-death model with just 2 parameters:\n\nmcmc_fit &lt;- mcmc(loglik, pars, nsteps = 1000, w=1) \n\nLet’s use an ensemble MCMC sampler.\nThe make.bisse function will throw errors if we try to plug in negative parameters:\n\nloglik(-pars)\n\nFor an MCMC routine that isn’t included in diversitree, we need to ensure that we can do an unrestricted search in parameter space.\n\nloglik_transformed &lt;- function(pars){\n    pars &lt;- exp(pars)\n    loglik &lt;- loglik(pars)\n    return(loglik)\n}\n\nloglik_transformed(log(pars))\n# This looks ok.\n\nloglik_transformed(-log(pars))\n# Calculates a value without error. Good to go.\n\nnwalkers &lt;- 100 #ensemble size\nnsteps &lt;- 50 #number of times to update entire nsemble\n\n# initialize the ensemble:\nmcmc_inits &lt;- matrix(nrow=nwalkers, ncol=6)\nfor(i in 1:nwalkers) mcmc_inits[i,] &lt;- jitter(log(mlepars))\n\n# run the ensemble MCMC:\nmcmc_fit &lt;- MCMCEnsemble(loglik_transformed, \n    inits=mcmc_inits, \n    max.iter=nwalkers*nsteps, \n    n.walkers = nwalkers)\n\n# Now, let's try to visualize the profile log-likelihoods (including\n#   the curves we generated earlier):\n\npar(mfrow=c(3,2))\n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,1],xlab=bquote(lambda[0]))\n#lines(log(lambdavals), loglik_lambda0_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,2],xlab=bquote(lambda[1]))\n#lines(log(lambdavals), loglik_lambda1_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,3],xlab=bquote(mu[0]))\n#lines(log(muvals), loglik_mu0_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,4],xlab=bquote(mu[1]))\n#lines(log(muvals), loglik_mu1_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,5],xlab=bquote(q[0~1]))\n#lines(log(qvals), loglik_q01_vals, col='red') \n\nplot(mcmc_fit$log.p ~ mcmc_fit$samples[,,6],xlab=bquote(q[1~0]))\n#lines(log(qvals), loglik_q10_vals, col='red') \n\nIn the vicinity of the MLE, the MCMC results should closely match the profile likelihood curves (but far away from the MLE there is no reason why they should be similar). (Why?)\nLooking at the different plots, are you able to say which parameters that are more difficult to estimate than others?\nWhat if we look at parameter correlations?\n\nplot(mcmc_fit$samples[,,1] ~ mcmc_fit$samples[,,2],\n    xlab=bquote(lambda[1]),\n    ylab=bquote(lambda[0]))"
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#section",
    "href": "topics/sse_models/fit_bisse.html#section",
    "title": "Fitting BiSSE models:",
    "section": "1.",
    "text": "1.\nWhat parameters seem to exhibit the highest correlations? The lowest? (You will need to plot all the pairwise combinations.) How do parameter estimates compare to the values used in the simulation? Is it easier to estimate one diversification rate or the other? Ditto for the mu’s and q’s. Why do you think that is?"
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#section-1",
    "href": "topics/sse_models/fit_bisse.html#section-1",
    "title": "Fitting BiSSE models:",
    "section": "2.",
    "text": "2.\nIncrease the number of taxa in your tree. How does your ability to estimate parameters change? Does it become easier to estimate migration rates, or death rates? Using 200 taxa should be doable, but see how high you can go before computations get bogged down. Be sure to describe bias as well as precision of estimates."
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#section-2",
    "href": "topics/sse_models/fit_bisse.html#section-2",
    "title": "Fitting BiSSE models:",
    "section": "3.",
    "text": "3.\nYou may notice that the “data” (putting it in quotes b/c we simulated it) contain more information about one of the migration rates than the other. Which migration rate is it, and why do you think it might be easier to estimate it with greater precision than the other one? It might be helpful to plot the tree.\n\nplot(history.from.sim.discrete(tree, states=c(0,1)),\n                tree, col=c('0'='black','1'='red'))"
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#section-3",
    "href": "topics/sse_models/fit_bisse.html#section-3",
    "title": "Fitting BiSSE models:",
    "section": "4.",
    "text": "4.\nSet migrations to be the same order of magnitude as the the diversification rates and repat the analysis. Does it always become easier to estimate q_ij?"
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#section-4",
    "href": "topics/sse_models/fit_bisse.html#section-4",
    "title": "Fitting BiSSE models:",
    "section": "5.",
    "text": "5.\nSet migration to be asymmetric. How easy is this to detect? You can start by assuming diversification and extinction rates are the same in each location."
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#section-5",
    "href": "topics/sse_models/fit_bisse.html#section-5",
    "title": "Fitting BiSSE models:",
    "section": "6.",
    "text": "6.\nUnder what circumstances do you think it is possible to reliably estimate extinction rates? Carry out an analysis to confirm or refute your hypothesis."
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#section-6",
    "href": "topics/sse_models/fit_bisse.html#section-6",
    "title": "Fitting BiSSE models:",
    "section": "7.",
    "text": "7.\nAre there are any situations where it is difficult to estimate diversification rates? Comment on bias and variance."
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#section-7",
    "href": "topics/sse_models/fit_bisse.html#section-7",
    "title": "Fitting BiSSE models:",
    "section": "8.",
    "text": "8.\nNaively, one might think that having larger differences between parameters (lambda0 vs lambda1, for example) would make estimation easier. Why is that not necessarily the case with BiSSE models? Think back to aeons ago when we simulated these models under different parameter combinations…"
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#section-8",
    "href": "topics/sse_models/fit_bisse.html#section-8",
    "title": "Fitting BiSSE models:",
    "section": "9.",
    "text": "9.\nTry setting a constraint in the log-likelihood function that mu0=mu1. Does this help estimate the other parameters? Does it make it easier to estimate the overall exctinction rate (mu)?\nHint: it works like this: loglik_constrained &lt;- constrain(loglik, mu0 ~ mu1)"
  },
  {
    "objectID": "topics/sse_models/fit_bisse.html#probably-hard",
    "href": "topics/sse_models/fit_bisse.html#probably-hard",
    "title": "Fitting BiSSE models:",
    "section": "10. (Probably hard)",
    "text": "10. (Probably hard)\nLoad the phangorn package, and use the nni function to get all of the trees that are one nni move away from your simulated tree. Evaluate the likelihood of all of the trees (but just use the MLE for the other parameters). Does the true tree carry the highest likelihood? Do you think the tree itself is identifiable? (Hint: use the chronos function to produce ultrametric trees from the nni output)."
  },
  {
    "objectID": "topics/sse_models/sim_sse.html",
    "href": "topics/sse_models/sim_sse.html",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "",
    "text": "rm(list=ls())\nlibrary(diversitree)\n\nState-dependent speciation and extion (SSE) models are the subject of this set of exercises. The workhorse of these methods are the various flavors of birth-death models for generating trees.\nSo far, we have been treating the trees as given without asking how they are created (by some biologist, presumably). If the traits we are interested in do not influence diversification, extinction/death, or the probability we observe a particular lineage, then the standard Markov models on trees seem sufficient. In some cases, traits might influence the process creating the phylogenetic relationships we observe, and we would like to detect these effects.\nIt is for this reason that we start by considering models for tree generation (sometimes called “tree priors” by people who use BEAST, which we will get to later in the week).\nLet’s start by simulating some trees using birth-death models.\n\ntree &lt;- tree.bd(c(lambda=1, mu=0), max.taxa = 50)\nplot(tree)\n\nNeat-o. But it doesn’t always work if mu&gt;0. Let’s use the lapply function to generate a list of simulated outputs (lapply is like a for loop, but more high-brow).\n\nnumtrees &lt;- 50\ntreelist &lt;- lapply(1:numtrees, \n        function(x) tree.bd(c(lambda=10, mu=5), max.taxa = 50)\n)\n\ndieoffs &lt;- sapply(treelist, is.null)\nnumdieoffs &lt;- sum(dieoffs)\n\nSometimes we fail to generate a tree with max.taxa number of lineages because they all go extinct (this is stochastic). Let’s see how many we were able to generate:"
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#learning-by-simulating",
    "href": "topics/sse_models/sim_sse.html#learning-by-simulating",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "",
    "text": "rm(list=ls())\nlibrary(diversitree)\n\nState-dependent speciation and extion (SSE) models are the subject of this set of exercises. The workhorse of these methods are the various flavors of birth-death models for generating trees.\nSo far, we have been treating the trees as given without asking how they are created (by some biologist, presumably). If the traits we are interested in do not influence diversification, extinction/death, or the probability we observe a particular lineage, then the standard Markov models on trees seem sufficient. In some cases, traits might influence the process creating the phylogenetic relationships we observe, and we would like to detect these effects.\nIt is for this reason that we start by considering models for tree generation (sometimes called “tree priors” by people who use BEAST, which we will get to later in the week).\nLet’s start by simulating some trees using birth-death models.\n\ntree &lt;- tree.bd(c(lambda=1, mu=0), max.taxa = 50)\nplot(tree)\n\nNeat-o. But it doesn’t always work if mu&gt;0. Let’s use the lapply function to generate a list of simulated outputs (lapply is like a for loop, but more high-brow).\n\nnumtrees &lt;- 50\ntreelist &lt;- lapply(1:numtrees, \n        function(x) tree.bd(c(lambda=10, mu=5), max.taxa = 50)\n)\n\ndieoffs &lt;- sapply(treelist, is.null)\nnumdieoffs &lt;- sum(dieoffs)\n\nSometimes we fail to generate a tree with max.taxa number of lineages because they all go extinct (this is stochastic). Let’s see how many we were able to generate:"
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section",
    "href": "topics/sse_models/sim_sse.html#section",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "1.",
    "text": "1.\nThe parameter “lambda” is the birth/diversification rate, and the parameter “mu” is the mortality/extinction rate. Try different combinations of the parameters (try lambda = 20, 50, 100, and mu = 20, 50, 100). What do you notice about your simulations? Hint: you should write a function called “getnumdieoffs” that accepts lambda, mu, and numtaxa, and then spits out treelist. Then you can create another function that acts on treelist to return numdieoffs. Organize the results of your simulations in a nice display."
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section-1",
    "href": "topics/sse_models/sim_sse.html#section-1",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "2.",
    "text": "2.\nTry to simulate trees with lambda = 1, mu = 2, and 10 taxa. How many simulations do you need to run until you start seeing trees with 10 taxa produced?"
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section-2",
    "href": "topics/sse_models/sim_sse.html#section-2",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "3.",
    "text": "3.\nUse ?trees to examine the help page for some of other tree simulation models. You will notcie tree.bd and tree.yule toward the bottom. Use your investigative abilities to research the Yule process and compare it to the birth- death process."
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section-3",
    "href": "topics/sse_models/sim_sse.html#section-3",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "4.",
    "text": "4.\nDo tree.yule(1, max.taxa=10) and tree.bd(lambda=2,mu=1, max.taxa=10) simulate the same process? Justify your answer."
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section-4",
    "href": "topics/sse_models/sim_sse.html#section-4",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "5.",
    "text": "5.\nDo tree.yule(1, max.taxa=10) and tree.bd(lambda=1,mu=0, max.taxa=10) simulate the same process? Justify your answer."
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section-5",
    "href": "topics/sse_models/sim_sse.html#section-5",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "6.",
    "text": "6.\nThe diversitree package can simulate tip data using Markov models for two discrete states initialized from a particular state at the root. One model is the “mk2” model: sim.character(tree, pars, x0=0, model=“mk2”, br=NULL) the “pars” argument is a vector of the form (q12,q21) corresponding to the transition rates of the stochastic rate matrix, Q (the model argument can also be set to “mkn” to simulate Markov models with n states). Compare sim.character with the simMk function from phytools. Confirm whether these two R functions simulate the same process."
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#bisse-simulation-exercises",
    "href": "topics/sse_models/sim_sse.html#bisse-simulation-exercises",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "BiSSE Simulation Exercises:",
    "text": "BiSSE Simulation Exercises:"
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section-6",
    "href": "topics/sse_models/sim_sse.html#section-6",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "1 .",
    "text": "1 .\nSimulate a population that starts in a hostile environment but can move to a safer habitat with less predation. Assume the organisms are dumb (or just plants), so that individuals tend to spend the same amount of time in each habitat before moving to the other one. Do the simulations match your intuition about what should happen to the population in this scenario? Do you see different outcomes if migration is slow or fast relative to the birth and death rates? What happens if you simulate more taxa? (Try up to 10^4 or 10^5)."
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section-7",
    "href": "topics/sse_models/sim_sse.html#section-7",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "2.",
    "text": "2.\nSimulate a population that can move to a new habitat where there are more resources available, and organisms can produce more offspring. Assume that organisms have the same life expectancy in each area. You can also assume that individuals spend the same amount of time in each location. How does this scenario compare to the previous one, in terms of the trees you generate? Try different numbers of taxa."
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section-8",
    "href": "topics/sse_models/sim_sse.html#section-8",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "3.",
    "text": "3.\nNow, suppose through no fault of their own that organisms are transported to a terrible environment at a much faster rate than they can escape. If you didn’t know the parameters you used, would it be obvious from the trees that one of the locations is worse than the the other?"
  },
  {
    "objectID": "topics/sse_models/sim_sse.html#section-9",
    "href": "topics/sse_models/sim_sse.html#section-9",
    "title": "Intro to State-dependent speciation and extinction (SSE) models",
    "section": "4.",
    "text": "4.\nSupose the organisms are intelligent, and preferentially move to the location where there is less predation, and/or better resources. Do the phylogenies contain information about the crummy habitat?"
  },
  {
    "objectID": "topics/ancestral_character_estimation/ace_sims.html",
    "href": "topics/ancestral_character_estimation/ace_sims.html",
    "title": "Fitting and simulating Markov models on trees using ancestral character estimation (ace)",
    "section": "",
    "text": "rm(list=ls())\n\nlibrary(ape)\nlibrary(expm) # matrix exponential\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'expm'\n\n\nThe following object is masked from 'package:Matrix':\n\n    expm\n\n\nThe following object is masked from 'package:ape':\n\n    balance\n\n\nThis code simulates output from the ace function. We will first try to grasp how the model behaves before we we try fitting it to data.\nAncestral character estimation methods can be based on a variety of approaches, but we will consider Markov chains superimposed on phylogenies. In the accompanying code to this we will learn how to fit these models using Maximum Likelihood, but for now we focus on simulating Markov chains on trees to understand how the models work.\nFirst, let’s simulate a tree and some tip states:\n\ntree &lt;- rcoal(10)\nx &lt;- sample(c(0,1), size=10, replace=T)\n\nplot(tree)\ntiplabels(pch=21,bg=x)\n\n\n\n\n\n\n\nfit &lt;- ace(x, tree, type='discrete', model='ARD')\n\nWarning in sqrt(diag(solve(h))): NaNs produced"
  },
  {
    "objectID": "topics/ancestral_character_estimation/ace_sims.html#section",
    "href": "topics/ancestral_character_estimation/ace_sims.html#section",
    "title": "Fitting and simulating Markov models on trees using ancestral character estimation (ace)",
    "section": "1.",
    "text": "1.\nSimulate some tip states on a tree using a Markov model that you specify, and use ace to estimate it. Scale this up for a large number of tip state configurations to see how the distribution of Maximum Likelihood estimates for the Markov model rates compare with the true values you supplied. Are there particular combinations of branch lengths and rates that make it difficult to estimate model parameters?"
  },
  {
    "objectID": "topics/ancestral_character_estimation/ace_sims.html#hard",
    "href": "topics/ancestral_character_estimation/ace_sims.html#hard",
    "title": "Fitting and simulating Markov models on trees using ancestral character estimation (ace)",
    "section": "2. (Hard)",
    "text": "2. (Hard)\nModify the code above to condition on a particular tip state configuration. This way, we can observe distributions of evolutionary scenarios consistent with a particular set of observations."
  },
  {
    "objectID": "topics/ancestral_character_estimation/ace_sims.html#section-1",
    "href": "topics/ancestral_character_estimation/ace_sims.html#section-1",
    "title": "Fitting and simulating Markov models on trees using ancestral character estimation (ace)",
    "section": "3.",
    "text": "3.\nIn a fitted ace object, there is an elemet called lik.anc which records the probability of ancestral states at each node of the tree. Plot a dataset of your choice with pie charts displaying the ancestral state probabilities associated with models that you fit using ace. Navigate through the help pages (?ace) to specify a few different versions of models that impose constraints on the Markov generator, Q. How sensitive are ancestral state probabilities to your modeling choices?"
  },
  {
    "objectID": "topics/character_mapping/fit_stochastic_mapping.html",
    "href": "topics/character_mapping/fit_stochastic_mapping.html",
    "title": "Exercise set 1:",
    "section": "",
    "text": "rm(list=ls())\nlibrary(coda) # for handling output from MCMC\nlibrary(phytools) # simmap functions, Mk models\n\nThis code is complementary to the other one in this subdirectory, and walks through how to fit models using a character mapping approach.\nMost of the phytools stuff is based on the Mk model, which is a particular type of Markov model. M stands for “Markov”, and “k” indicates the number of states. (Lewis has an interesting discussion relating to the use of conditional likelihoods as a means of overcoming acquisition bias – the tendency to only record changes of state, and not maintenance of a character).\nfitMk fits the model to the data using nlminb, and optimization routine similar to quasi- Newton methods like BFGS. Compare this to ace.\nmcmcMk fits Mk models to trees using MCMC.\n\ndata(sunfish.tree)\ndata(sunfish.data)\n\n# extract discrete character (fish diet):\nsunfish_tipdata &lt;-setNames(sunfish.data$feeding.mode,\n    rownames(sunfish.data))\n\n# fit \"ER\" model\nfit.ER&lt;-fitMk(sunfish.tree,sunfish_tipdata,model=\"ER\")\nprint(fit.ER)\n\nThis will run for a while, and you should see traceplots updating in real time until it finishes:\n\n    fit.mcmc &lt;- mcmcMk(sunfish.tree, sunfish_tipdata, \n    model=\"ER\", ngen=10000)\n\nThe dimensions of fit.mcmc will be ngen * (number of parameters in the Mk model + 2), which for this run will be 10000 x 3 because we are only estimating a single parameter\nThe first column of fit.mcmc is timesteps of the MCMC routine. The last column of fit.mcmc is the logLikelihood, and the columns between first and last are the parameters at each timestep.\nWe can look at the traceplots:\n\nplot(fit.mcmc)\n\nWe can also plot the posterior distribution for our parameters. By default, this should toss out the first 20% of the MCMC updates as part of a burn-in (we discard transients because we only care about the stationary distribution of the MCMC run)\n\nplot(density(fit.mcmc))\n\nOf note, we can also use base R plotting to accomplish these things as well. Here is a histogram of the parameter values of the entire MCMC run:\n\nhist(fit.mcmc[,'[pisc,non]'],breaks=100) \n\n# and the last 80% of the run:\nhist(fit.mcmc[c(2001:10000),'[pisc,non]'],breaks=100) \n\nI also like to plot profile log- likelihood (logLik~parameters):\n\nplot(fit.mcmc[,'logLik']~fit.mcmc[,'[pisc,non]'])\n\nThis helps us visualize the peak of the likelihood surface. Can be useful for diagnosing convergence problems."
  },
  {
    "objectID": "topics/character_mapping/fit_stochastic_mapping.html#fitting-markov-models-with-stochastic-character-mapping",
    "href": "topics/character_mapping/fit_stochastic_mapping.html#fitting-markov-models-with-stochastic-character-mapping",
    "title": "Exercise set 1:",
    "section": "",
    "text": "rm(list=ls())\nlibrary(coda) # for handling output from MCMC\nlibrary(phytools) # simmap functions, Mk models\n\nThis code is complementary to the other one in this subdirectory, and walks through how to fit models using a character mapping approach.\nMost of the phytools stuff is based on the Mk model, which is a particular type of Markov model. M stands for “Markov”, and “k” indicates the number of states. (Lewis has an interesting discussion relating to the use of conditional likelihoods as a means of overcoming acquisition bias – the tendency to only record changes of state, and not maintenance of a character).\nfitMk fits the model to the data using nlminb, and optimization routine similar to quasi- Newton methods like BFGS. Compare this to ace.\nmcmcMk fits Mk models to trees using MCMC.\n\ndata(sunfish.tree)\ndata(sunfish.data)\n\n# extract discrete character (fish diet):\nsunfish_tipdata &lt;-setNames(sunfish.data$feeding.mode,\n    rownames(sunfish.data))\n\n# fit \"ER\" model\nfit.ER&lt;-fitMk(sunfish.tree,sunfish_tipdata,model=\"ER\")\nprint(fit.ER)\n\nThis will run for a while, and you should see traceplots updating in real time until it finishes:\n\n    fit.mcmc &lt;- mcmcMk(sunfish.tree, sunfish_tipdata, \n    model=\"ER\", ngen=10000)\n\nThe dimensions of fit.mcmc will be ngen * (number of parameters in the Mk model + 2), which for this run will be 10000 x 3 because we are only estimating a single parameter\nThe first column of fit.mcmc is timesteps of the MCMC routine. The last column of fit.mcmc is the logLikelihood, and the columns between first and last are the parameters at each timestep.\nWe can look at the traceplots:\n\nplot(fit.mcmc)\n\nWe can also plot the posterior distribution for our parameters. By default, this should toss out the first 20% of the MCMC updates as part of a burn-in (we discard transients because we only care about the stationary distribution of the MCMC run)\n\nplot(density(fit.mcmc))\n\nOf note, we can also use base R plotting to accomplish these things as well. Here is a histogram of the parameter values of the entire MCMC run:\n\nhist(fit.mcmc[,'[pisc,non]'],breaks=100) \n\n# and the last 80% of the run:\nhist(fit.mcmc[c(2001:10000),'[pisc,non]'],breaks=100) \n\nI also like to plot profile log- likelihood (logLik~parameters):\n\nplot(fit.mcmc[,'logLik']~fit.mcmc[,'[pisc,non]'])\n\nThis helps us visualize the peak of the likelihood surface. Can be useful for diagnosing convergence problems."
  },
  {
    "objectID": "topics/character_mapping/fit_stochastic_mapping.html#section",
    "href": "topics/character_mapping/fit_stochastic_mapping.html#section",
    "title": "Exercise set 1:",
    "section": "1.",
    "text": "1.\nDid the fitMk routine produce the same estimates as mcmcMk? How would you determine this?"
  },
  {
    "objectID": "topics/character_mapping/fit_stochastic_mapping.html#section-6",
    "href": "topics/character_mapping/fit_stochastic_mapping.html#section-6",
    "title": "Exercise set 1:",
    "section": "1.",
    "text": "1.\nWhy do you think the mcmcMk routine does not propose character mappings alongside parameters? Would anything be gained by treating character mappings as latent variables in MCMC and updating those alongside parameters?\nCheck this paper out if you are interested in MCMC and character mapping:\nMutations as Missing Data: Inferences on the Ages and Distributions of Nonsynonymous and Synonymous Mutations, by Rasmus Nielsen Genetics 159: 401-411 (September 2001)"
  },
  {
    "objectID": "topics/character_mapping/fit_stochastic_mapping.html#section-7",
    "href": "topics/character_mapping/fit_stochastic_mapping.html#section-7",
    "title": "Exercise set 1:",
    "section": "2.",
    "text": "2.\nComment on any interesting things you see in profile loglikelihoods for models with multiple parameters. Can you deduce anything interesting about the geometry of the likelihood surface?"
  },
  {
    "objectID": "topics/character_mapping/fit_stochastic_mapping.html#section-8",
    "href": "topics/character_mapping/fit_stochastic_mapping.html#section-8",
    "title": "Exercise set 1:",
    "section": "3.",
    "text": "3.\nIn the previous runs, how did we estimate Pi, the probability of the root states? Examine the fitMk and mcmcMk help page to determine what the default is. Try fitting the model under a different setting to see how results change."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Workshop",
    "section": "",
    "text": "This workshop is organized and hosted by MAGPIE.\nIt is focused toward our graduate students, but anybody with interests in mathematical biology, evolutionary biology, infectious diseases, and statistics is welcome to join!"
  },
  {
    "objectID": "about.html#instructors",
    "href": "about.html#instructors",
    "title": "About This Workshop",
    "section": "Instructors",
    "text": "Instructors\n\n\n\n\n\n\nInstructor One\nShort bio paragraph: background, expertise, and current role.\nOptional links: Personal Website, GitHub.\n\n\n\n\n\n\n\n\n\n\n\n\nInstructor Two\nShort bio paragraph: background, teaching areas, and research focus.\nOptional links: Personal Website."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAGPIE PhylogeogRaphy Workshop (with BEAST 2)",
    "section": "",
    "text": "Sessions are split into roughly two-hour chunks that align with typical start/end times of classes, and are meant to be as self-contained as possible – so, feel free to come and go as necessary! All materials will posted here for you to catch up on content if desired.\nParticipants should bring their own laptops for hands-on exercises.\nFor specific requirements, see the individual session pages.\n\n\n\nMonday[September 29, 2025]\n\n\n\n\n\n0930–1130: Introduction to Phylogenies and Markov models\n\nShort description\n\n\n\n\n\n\n\n1130–1430: Basic models for ancestral character estimation\n\nShort description\n\n\n\n\n\n\n\n\n1430–1630: Intro to Stochastic character mapping\n\nShort description\n\n\n\n\n\n\n\n1430–1630: Fitting models with Stochastic character mapping\n\nShort description\n\n\n\n\n\n\nWednesday[October 1, 2025]\n\n\n\n\nIntro to State-dependent Speciation and Extinction (SSE) models\n\nShort description\n\n\n\n\n\nFitting basic birth-death models\n\nDisplay and embed PDFs\n\n\n\n\n\nFitting BiSSE models\n\nDisplay and embed PDFs\n\n\n\n\n\nAncestral state reconstruction with BiSSE\n\nDisplay and embed PDFs\n\n\n\n\n\n\nThursday[October 2, 2025]\n\n\n\n\n\nIntro to Phylodynamics\n\nThis is about embedding R code\n\n\n\n\n\nModels in BEAST2\n\nSame as R but with Python\n\n\n\n\n\n\n\n\n\nFriday[October 3, 2025]\n\n\n\n\n\nAdvanced BEAST2 topics: BSSVS\n\nThis is about embedding R code\n\n\n\n\n\nModels in BEAST2\n\nSame as R but with Python\n\n\n\n\n\n\n\n \n\n\nHosted by:"
  },
  {
    "objectID": "topics/character_mapping/stochastic_mapping.html",
    "href": "topics/character_mapping/stochastic_mapping.html",
    "title": "Simulating stochasting character mappings",
    "section": "",
    "text": "This code simulates stochastic character mappings on a fixed phylogenetic tree. In the real world, we would be given a phylogeny with tip data (diets of finches, geographic location of influenza, etc.). The goal would be to estimate quantities like the number of changes, the timing of changes, whether one trait is ancestral to the rest, and so on. This would involve fitting models to estimate the stochastic rate matrix describing discrete state changes, as well as the root states.\nIn this code, we are going to start by exploring how we would interpret estimates. This involves simulating character mappings on the phylogeny using the true values of the Markov chain model, and our goal is to see if we can count the number of state changes, describe timing of changes, etc.\nOnce we have a feel for how the model works, we will then try to do the harder task of fitting it to data.\nClear out the console:\n\nrm(list=ls())\n\n# load in the package for stochastic character mapping:\nlibrary(phytools)\n\nLoading required package: ape\n\n\nLoading required package: maps\n\n# Set the number of stochastic character mappings to simulate:\nnsim = 1000\n\n# Create a tree: (could modify this to load in a tree as well)\ntree &lt;- pbtree(n=50, scale=1)\n\nSpecify a transition rate matrix for two states (A and B). In practice, we would want to estimate this from data. Let’s pretend we’ve done that step, and this can be taken as known without error:\n\nQ &lt;- matrix(c(-1,1,\n              1,-1),2,2)\nrownames(Q) &lt;- colnames(Q) &lt;- c(\"A\",\"B\")\n\nWe can scale down Q to have fewer transitions (for illustrative purposes)\n\nQ &lt;- Q/10\n\nSimulate tip states on the tree using our stochastic rate matrix Q. In practice, this would be observed in data, and in many instances we could assume it is observed without error. For our purposes here, we simulate tip states from the model:\n\ntip_states &lt;- sim.Mk(tree, Q)\n\nHow many of each state do we oberve?\n\ntable(tip_states)\n\ntip_states\n A  B \n43  7 \n\n\nSimulate stochastic character maps nsim times using the tree, tip states, and our Q matrix: Note: don’t set model=Q, otherwise that just re-estimates Q. Need to specify Q=Q in the make.simmap call.\nWe simulate from the root to the tips. We have to specify an initial condition at the root:\n\nPi &lt;- c(.5,.5)\n\n# Run the sims!\nsims &lt;- make.simmap(tree, tip_states, Q=Q, pi=Pi, nsim=nsim)\n\nmake.simmap is sampling character histories conditioned on\nthe transition matrix\n\nQ =\n     A    B\nA -0.1  0.1\nB  0.1 -0.1\n(specified by the user);\nand (mean) root node prior probabilities\npi =\n  A   B \n0.5 0.5 \n\n\nDone.\n\n# Uncomment this to visualize the different character mappings:\n#par(mfrow=c(1,1))\n#for(i in 1:nsim){\n#   plot(sims[[i]])\n#   Sys.sleep(.0002)\n#}\n\n# Let's start by summarizing the number of mutations that happen across the whole tree:\n\ncounts &lt;- lapply(sims, countSimmap)\nnumMutations &lt;- sapply(counts, function(x) x$N)\ntransitions &lt;- lapply(counts, function(x)x$Tr)\ntransitions_AB &lt;- sapply(transitions, function(x) x[1,2])\ntransitions_BA &lt;- sapply(transitions, function(x) x[2,1])\n\nLet’s plot an example character mapped tree, and histograms of number of mutations:\n\nsim = sims[[1]]\n#  overall, and by type\npar(mfrow=c(2,2))\n## plot the character mapped tree\nsim$tip.label &lt;- paste(sim$tip.label, \"(\", tip_states, \")\", sep=\"\")\nplotSimmap(sim, fsize = 0.8)\n\nno colors provided. using the following legend:\n        A         B \n  \"black\" \"#DF536B\" \n\n## plottig make.simmap objects messes up margins, let's reset these:\npar(mar=c(5,4,4,1))\n## histogram of total mutations:\nhist(numMutations)\n## histogram of mutations A -&gt; B\nhist(transitions_AB)\n## histogram of mutations B -&gt; A\nhist(transitions_BA)"
  },
  {
    "objectID": "topics/character_mapping/stochastic_mapping.html#section",
    "href": "topics/character_mapping/stochastic_mapping.html#section",
    "title": "Simulating stochasting character mappings",
    "section": "1.",
    "text": "1.\nTry changing the initial condition at the root (Pi, also called the “root prior”) to see how results change"
  },
  {
    "objectID": "topics/character_mapping/stochastic_mapping.html#section-1",
    "href": "topics/character_mapping/stochastic_mapping.html#section-1",
    "title": "Simulating stochasting character mappings",
    "section": "2.",
    "text": "2.\nLook at the logL values in the sims. What do you think that describes? In particular, do you think the reported logL is P(tip data | Q), or P(tip data, character mapping | Q)?"
  },
  {
    "objectID": "topics/character_mapping/stochastic_mapping.html#a.",
    "href": "topics/character_mapping/stochastic_mapping.html#a.",
    "title": "Simulating stochasting character mappings",
    "section": "2.a.",
    "text": "2.a.\nCreate a new sims list, replacing Q with jitter(Q). What do you notice about the logL values?"
  },
  {
    "objectID": "topics/character_mapping/stochastic_mapping.html#section-2",
    "href": "topics/character_mapping/stochastic_mapping.html#section-2",
    "title": "Simulating stochasting character mappings",
    "section": "3.",
    "text": "3.\nIn light of the previous questions, can you figure out what the form of the log-likelihood is for make.simmap? (is it actually a likelihood, or is it a conditional likelihood?) That is, do you think the form of the log-likelihood is P(tip data | Q) or P(tip data | Q, Pi) ?"
  },
  {
    "objectID": "topics/character_mapping/stochastic_mapping.html#section-3",
    "href": "topics/character_mapping/stochastic_mapping.html#section-3",
    "title": "Simulating stochasting character mappings",
    "section": "4.",
    "text": "4.\nCalculate the log-likelihood of the tip data on the tree under the assumed Q matrix. Compare the log-likelihood reported from ace with the log-likelihood returned by make.simmap. What do you notice?"
  },
  {
    "objectID": "topics/character_mapping/stochastic_mapping.html#section-4",
    "href": "topics/character_mapping/stochastic_mapping.html#section-4",
    "title": "Simulating stochasting character mappings",
    "section": "5.",
    "text": "5.\nDo ace and make.simmap use the same log-likelihood? If not, what is the key difference? Use ?ace and ?make.simmap in your investigation."
  },
  {
    "objectID": "topics/ancestral_character_estimation/nucleotide_models.html",
    "href": "topics/ancestral_character_estimation/nucleotide_models.html",
    "title": "1. Basic Markov chains",
    "section": "",
    "text": "rm(list=ls())\nlibrary(ape)\nlibrary(expm) # matrix exponential\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'expm'\n\n\nThe following object is masked from 'package:Matrix':\n\n    expm\n\n\nThe following object is masked from 'package:ape':\n\n    balance\nThis code simulates output from the ace function. We will first try to grasp how the model behaves before we we try fitting it to data.\nAncestral character estimation methods can be based on a variety of approaches, but we will consider Markov chains superimposed on phylogenies. In the accompanying code to this we will learn how to fit these models using Maximum Likelihood, but for now we focus on simulating Markov chains on trees to understand how the models work."
  },
  {
    "objectID": "topics/ancestral_character_estimation/nucleotide_models.html#section",
    "href": "topics/ancestral_character_estimation/nucleotide_models.html#section",
    "title": "1. Basic Markov chains",
    "section": "1.",
    "text": "1.\nWrite down a 4-state Markov model corresponding to A, C, T, and G in DNA. Evaluate the transition probabilities as above (hint: the matrix exponential can be calculated in R using the expm function from the package of the same name. Feel free to assume whatever constraints on parameters you like (or none at all)."
  },
  {
    "objectID": "topics/ancestral_character_estimation/nucleotide_models.html#section-1",
    "href": "topics/ancestral_character_estimation/nucleotide_models.html#section-1",
    "title": "1. Basic Markov chains",
    "section": "2.",
    "text": "2.\nThe Jukes-Cantor model is the simplest nucleotide substitution model. By assumption, the equilibrium probabilities of each nucleotide are equal, and the transition rates between states are likewise all equal. What is the generator for this process? Write a function to describe transition probabilities as a function of time. (Hint: how many parameters will your generator have if all transitions between states occur at equal rates?)"
  },
  {
    "objectID": "topics/ancestral_character_estimation/nucleotide_models.html#section-2",
    "href": "topics/ancestral_character_estimation/nucleotide_models.html#section-2",
    "title": "1. Basic Markov chains",
    "section": "3.",
    "text": "3.\nThe K80 (Kimura, 1980) model elaborates on the Jukes-Cantor model by allowing substitutions within pyrimidines (C,T) and within purines (A, G) to occur at a different rate than “transversions” between pyrimidines and purines. In biology, substitutions between two purines or between two pyrimidines are called “transitions” (but in Markov chains, mathematicians refer to all state changes as transitions – confusing). Assuming that transitions (in the biological sense of the word) occur at the same rate regardless of whether the nucleotides are purines or pyrimidines, and assuming that transversions in either direction occur at the same rate, write the generator for the Markov process and write a function that calculates transition probabilities as a function of time."
  },
  {
    "objectID": "topics/sse_models/fit_bd.html",
    "href": "topics/sse_models/fit_bd.html",
    "title": "Estimating parameters of basic birth-death models",
    "section": "",
    "text": "Now that we have some familiarity with the behavior of the birth-death model, we will try to use the model for parameter inference, i.e. model fitting.\nModel fitting just consists of selecting parameters of the model so that the simulated outputs match our data as closely as possible. The dumbest way imaginable is to use the “eyeball test”. A more sophisticated approach is to use likelihoods.\nThe diversitree package has a convenient class of functions that can be used to define likelihood functions for different models. We will start with the simplest case - the birth death model.\n\n\nSimulate a list of bd trees here\n\ntree &lt;- tree.bd(c(lambda=5,mu=0), max.taxa = 20)\nloglik &lt;- make.bd(tree)\n\nThe make.bd function is a scalar function of two variables of the form loglik = loglik(c(lambda,mu))\nLet’s visualize the likelihood surface one parameter at a time (holding the other parameter at the true value):\n\nloglik_lambda &lt;- function(lambda) loglik(c(lambda,0))\nloglik_mu &lt;- function(mu) loglik(c(1,mu))\n\nlambdavals &lt;- exp( seq(-3,3,length=30))\nmuvals &lt;- exp( seq(-6,2,length=30))\nloglik_lambda_vals &lt;- sapply(lambdavals, loglik_lambda)\nloglik_mu_vals &lt;- sapply(muvals, loglik_mu)\n\nloglik_max &lt;- max(c(loglik_lambda_vals, loglik_mu_vals))\n\npar(mfrow=c(2,1))\nplot(lambdavals, loglik_lambda_vals, ylim=c(loglik_max-10,loglik_max+2))\nplot(muvals, loglik_mu_vals, ylim=c(loglik_max-10,loglik_max+2))\n\nThe maximum of the log-likelihood function roughly coincides with the parameters we used. How variable is this? Let’s simulate more trees:\n\ntreelist &lt;- trees(c(lambda=5,mu=0), \n        type=c(\"bd\"), n=100, \n        max.taxa=20)\n\n# make a log-likelihood function for each tree:\nlogliklist &lt;- lapply(treelist, make.bd )\n\nloglik_lambda_list &lt;- lapply(logliklist, function(x){\n            function(lambda) x(c(lambda,0))\n    })\n\nloglik_mu_list &lt;- lapply(logliklist, function(x){\n            function(mu) x(c(mu,0))\n    })\n\n\nlambdavals &lt;- exp( seq(-2,2,length=30))\nmuvals &lt;- exp( seq(-6,2,length=30))\n\nloglik_lambda_list_vals &lt;- sapply(loglik_lambda_list,\n    function(x){sapply(lambdavals, x)})\n\nloglik_mu_list_vals &lt;- sapply(loglik_mu_list,\n    function(x){sapply(muvals, x)})\n\nloglik_max &lt;- max(c(loglik_lambda_list_vals,loglik_mu_list_vals))\nloglik_min &lt;- min(c(loglik_lambda_list_vals,loglik_mu_list_vals))\n\n\npar(mfrow=c(2,1))\nplot(lambdavals, loglik_lambda_list_vals[,1]/abs(max(loglik_lambda_list_vals[,1])))\nfor(i in 2:100){\n    lines(lambdavals, loglik_lambda_list_vals[,i]/abs(max(loglik_lambda_list_vals[,i])))\n}\n\nplot(muvals, loglik_mu_list_vals[,1]/abs(max(loglik_mu_list_vals[,1])))\nfor(i in 2:100){\n    lines(muvals, loglik_mu_list_vals[,i]/abs(max(loglik_mu_list_vals[,i])))\n}\n\nSo, most of these have a maximum in the vicinity of the true parameter values that we suppied, but not all.\nThe “Maximum Likelihood Estimates” are the values of (lambda,mu) where the likelihood function is maximized. What does the distribution of MLEs look like for this set of trees?\n\nmles &lt;- lapply(logliklist, function(x){find.mle(x, c(1,0), method='subplex')})\n\nmles_pars &lt;- t(sapply(mles, function(x) x$par))\n\npar(mfrow=c(1,2))\nhist(mles_pars[,1],main='',\n    xlab=bquote(hat(lambda)))\nhist(mles_pars[,2],main='',\n    xlab=bquote(hat(mu)))\n\nRemember that the true value of mu we used was mu=0. A lot of the models suggest values of mu &gt; 0, which is interesting. This tells us that, in practice, we will probably not be able to reliably estimate mu when it is close to zero but positive.\nWe can frame a hypothesis test. Null hypothesis: mu=0. Alternative hypothesis: mu &gt; 0. Supposing we wish to keep false-positives at 5% or less, we could propose the statistical test that we reject the null hypothesis if our estimate of mu is at the 95-th percentile or greater of this “null” distribution. In our case, this critical value of mu would be approximately\n\nmu_critical &lt;- quantile(mles_pars[,'mu'], 0.95) \n\nSo in the future, if we estimate a value of mu greater than mu_critical, and we say “this appears significantly different from zero, therefore we reject the null hypothesis”, we will be making a type 1 error (“false-positive”) less than 5% of the time. A 1/20 chance of a mistake is reasonable in some circumstances, but less so in others."
  },
  {
    "objectID": "topics/sse_models/fit_bd.html#example-1-benchmarking-from-simulations",
    "href": "topics/sse_models/fit_bd.html#example-1-benchmarking-from-simulations",
    "title": "Estimating parameters of basic birth-death models",
    "section": "",
    "text": "Simulate a list of bd trees here\n\ntree &lt;- tree.bd(c(lambda=5,mu=0), max.taxa = 20)\nloglik &lt;- make.bd(tree)\n\nThe make.bd function is a scalar function of two variables of the form loglik = loglik(c(lambda,mu))\nLet’s visualize the likelihood surface one parameter at a time (holding the other parameter at the true value):\n\nloglik_lambda &lt;- function(lambda) loglik(c(lambda,0))\nloglik_mu &lt;- function(mu) loglik(c(1,mu))\n\nlambdavals &lt;- exp( seq(-3,3,length=30))\nmuvals &lt;- exp( seq(-6,2,length=30))\nloglik_lambda_vals &lt;- sapply(lambdavals, loglik_lambda)\nloglik_mu_vals &lt;- sapply(muvals, loglik_mu)\n\nloglik_max &lt;- max(c(loglik_lambda_vals, loglik_mu_vals))\n\npar(mfrow=c(2,1))\nplot(lambdavals, loglik_lambda_vals, ylim=c(loglik_max-10,loglik_max+2))\nplot(muvals, loglik_mu_vals, ylim=c(loglik_max-10,loglik_max+2))\n\nThe maximum of the log-likelihood function roughly coincides with the parameters we used. How variable is this? Let’s simulate more trees:\n\ntreelist &lt;- trees(c(lambda=5,mu=0), \n        type=c(\"bd\"), n=100, \n        max.taxa=20)\n\n# make a log-likelihood function for each tree:\nlogliklist &lt;- lapply(treelist, make.bd )\n\nloglik_lambda_list &lt;- lapply(logliklist, function(x){\n            function(lambda) x(c(lambda,0))\n    })\n\nloglik_mu_list &lt;- lapply(logliklist, function(x){\n            function(mu) x(c(mu,0))\n    })\n\n\nlambdavals &lt;- exp( seq(-2,2,length=30))\nmuvals &lt;- exp( seq(-6,2,length=30))\n\nloglik_lambda_list_vals &lt;- sapply(loglik_lambda_list,\n    function(x){sapply(lambdavals, x)})\n\nloglik_mu_list_vals &lt;- sapply(loglik_mu_list,\n    function(x){sapply(muvals, x)})\n\nloglik_max &lt;- max(c(loglik_lambda_list_vals,loglik_mu_list_vals))\nloglik_min &lt;- min(c(loglik_lambda_list_vals,loglik_mu_list_vals))\n\n\npar(mfrow=c(2,1))\nplot(lambdavals, loglik_lambda_list_vals[,1]/abs(max(loglik_lambda_list_vals[,1])))\nfor(i in 2:100){\n    lines(lambdavals, loglik_lambda_list_vals[,i]/abs(max(loglik_lambda_list_vals[,i])))\n}\n\nplot(muvals, loglik_mu_list_vals[,1]/abs(max(loglik_mu_list_vals[,1])))\nfor(i in 2:100){\n    lines(muvals, loglik_mu_list_vals[,i]/abs(max(loglik_mu_list_vals[,i])))\n}\n\nSo, most of these have a maximum in the vicinity of the true parameter values that we suppied, but not all.\nThe “Maximum Likelihood Estimates” are the values of (lambda,mu) where the likelihood function is maximized. What does the distribution of MLEs look like for this set of trees?\n\nmles &lt;- lapply(logliklist, function(x){find.mle(x, c(1,0), method='subplex')})\n\nmles_pars &lt;- t(sapply(mles, function(x) x$par))\n\npar(mfrow=c(1,2))\nhist(mles_pars[,1],main='',\n    xlab=bquote(hat(lambda)))\nhist(mles_pars[,2],main='',\n    xlab=bquote(hat(mu)))\n\nRemember that the true value of mu we used was mu=0. A lot of the models suggest values of mu &gt; 0, which is interesting. This tells us that, in practice, we will probably not be able to reliably estimate mu when it is close to zero but positive.\nWe can frame a hypothesis test. Null hypothesis: mu=0. Alternative hypothesis: mu &gt; 0. Supposing we wish to keep false-positives at 5% or less, we could propose the statistical test that we reject the null hypothesis if our estimate of mu is at the 95-th percentile or greater of this “null” distribution. In our case, this critical value of mu would be approximately\n\nmu_critical &lt;- quantile(mles_pars[,'mu'], 0.95) \n\nSo in the future, if we estimate a value of mu greater than mu_critical, and we say “this appears significantly different from zero, therefore we reject the null hypothesis”, we will be making a type 1 error (“false-positive”) less than 5% of the time. A 1/20 chance of a mistake is reasonable in some circumstances, but less so in others."
  },
  {
    "objectID": "topics/sse_models/fit_bd.html#section",
    "href": "topics/sse_models/fit_bd.html#section",
    "title": "Estimating parameters of basic birth-death models",
    "section": "1.",
    "text": "1.\nSuppose we want a more stringent statistical test for the previous example. What is the critical value of mu corresponding to a 1% chance of comitting a Type 1 error?"
  },
  {
    "objectID": "topics/sse_models/fit_bd.html#section-1",
    "href": "topics/sse_models/fit_bd.html#section-1",
    "title": "Estimating parameters of basic birth-death models",
    "section": "2.",
    "text": "2.\nModify the previous example to see how things change for a different value of lambda. Try lambda = 5. Does the critical value of mu change?"
  },
  {
    "objectID": "topics/sse_models/fit_bd.html#section-2",
    "href": "topics/sse_models/fit_bd.html#section-2",
    "title": "Estimating parameters of basic birth-death models",
    "section": "3.",
    "text": "3.\nThe Generalized Likelihood Ratio Test: another way to assess whether mu is significantly different from zero is to fit two versions of the log-Likelihood function: the one we already did, and a constrained version of the model with the constraint mu=0 hard-coded in (so that we just estimate lambda). The model with more parameters will always fit the data better, but it may not fit the data “that much” better. The GLT looks a log(ratio of likelihoods) = difference(log-Likelihoods) to produce a test statistic (also, rather annoyingly in this case, called lambda). Use constrain(loglik, mu ~ 0) to create the constrained log-likelihood functions, and fit them to obtain new estimates for lambda. Use Wikipedia, Google, or your favorite (work-safe) AI companion to help you formulate GLT test statistics and assess significant departues from the null hypothesis mu=0. Do the results of this statistical test match with those from earlier? If not, why not?\n(Hint: the GLR also makes an asymptotic approximation. What is that approximation, and under what situations would it apply here?)"
  },
  {
    "objectID": "topics/sse_models/fit_bd.html#section-3",
    "href": "topics/sse_models/fit_bd.html#section-3",
    "title": "Estimating parameters of basic birth-death models",
    "section": "4.",
    "text": "4.\nConsider a different null hypothesis: lambda &gt; mu. Develop a test statistic for this hypothesis, and identify critical values of your test statistic for 95% and 99% confidence regions. (Simulate large numbers of trees to obtain MLEs; don’t worry about GLR tests unless you want to)."
  },
  {
    "objectID": "topics/sse_models/fit_bd.html#section-4",
    "href": "topics/sse_models/fit_bd.html#section-4",
    "title": "Estimating parameters of basic birth-death models",
    "section": "5.",
    "text": "5.\nIf your simulation produced trees with estimated values of lambda &lt; mu, plot these trees and comment on any features they exhibit that stand out to you. If your simulation did not produce trees with MLEs having lambda &lt; mu, simulate as necessary until you find some."
  },
  {
    "objectID": "topics/sse_models/fit_bd.html#section-5",
    "href": "topics/sse_models/fit_bd.html#section-5",
    "title": "Estimating parameters of basic birth-death models",
    "section": "6.",
    "text": "6.\nThe find.mle function uses a variety of numerical optimization algorithms to find MLEs. There is also a function called “mcmc”. Use this function with nsteps=1000 and w=c(2,1). Plot loglikelihood (“p”) against parameters to visualize profile log-likelihoods, and also plot lambda against mu. Comment on what you see, and discuss implications of correlations between lambda and mu. (Note: there are other mcmc routines besides this one in diversitree. If you have a favorit method, feel free to use it.)"
  },
  {
    "objectID": "topics/sse_models/fit_bd.html#section-6",
    "href": "topics/sse_models/fit_bd.html#section-6",
    "title": "Estimating parameters of basic birth-death models",
    "section": "7.",
    "text": "7.\nThe previous exercises and examples used the max.taxa argument to condition on trees of a particular size. Flip a coin ( rbinom(1,1,0.5)). If it comes up heads (=1), change max.taxa to a different number and repeat Exercises 1-6. If it comes up tails (=0), replace max.taxa with max.t to simulate trees for a fixed amount of time (with variable numbers of taxa), and then repeat Exercises 1-6. (Caution: injudicious choices of lambda and max.t can make your computer explode. Ctrl + c is useful (on Mac, anyway) for cancelling calculations in the Terminal.)"
  },
  {
    "objectID": "topics/sse_models/asr_bisse.html",
    "href": "topics/sse_models/asr_bisse.html",
    "title": "Ancestral state reconstructions using BiSSE",
    "section": "",
    "text": "library(diversitree)\nlibrary(ape)\nlibrary(phytools)\n\nThe previous problem sets have illustrated the behavior of trees simulated from these models, some of their statistical behaviors, and now all that remains is to use fitted models to accomplish the goals of phylogeography, i.e. ancestral character estimation.\nNote: for most BiSSE users, the primary goal seems not to be ancestral reconstruction per se, but inferring relationships between states and diversification/extinction rates. As such, the documentation and support for the reconstruction methods that use BiSSE is more sparse than for ace or simmap.\nLet’s start by simulating a tree and fit the model to it. We will then infer ancestral states and compare these with the known values from the simulation itself.\nSimulate a BiSSE tree here:\n\npars &lt;- c(\n        lambda0=1,\n        lambda1=3,\n        mu0=0.1,\n        mu1=0.1,\n        q01=0.1,\n        q10=0.12)\n\ntree &lt;- tree.bisse(pars, max.taxa=100, x0=0)\n\npar(mfrow=c(1,2))\nplot(history.from.sim.discrete(tree, states=c(0,1)),\n    tree, col=c('0'='black','1'='red')) \n\nThe make.bisse function is a scalar function of two variables of the form loglik = loglik(c(lambda,mu))\n\nloglik &lt;- make.bisse(tree, tree$tip.state)\nloglikc &lt;- constrain(loglik, lambda0~lambda1)\n\n# What are the MLEs?\nmles &lt;- find.mle(loglik, pars)\nmlesc &lt;- find.mle(loglikc, pars[-2])\n\n\n# these are the parmeter values that this infers: \nmlepars &lt;- mles$par\n\n# We use the \"marginal\" reconstruction, which should be similar\n# to the type of calculation used by ace:\nasr_marginal &lt;- asr.marginal(loglik, mlepars)\n\nplot(tree, show.tip.label=F)\nnodelabels(pie=t(asr_marginal), piecol=c(\"black\", \"red\"), cex=0.7)\ntip_matrix &lt;- cbind(1-tree$tip.state, tree$tip.state)  # Convert to probabilities \ntiplabels(pie=tip_matrix, piecol=c(\"black\", \"red\"), cex=0.7)"
  },
  {
    "objectID": "topics/sse_models/asr_bisse.html#section",
    "href": "topics/sse_models/asr_bisse.html#section",
    "title": "Ancestral state reconstructions using BiSSE",
    "section": "1.",
    "text": "1.\nSimulate a collection of BiSSE trees (n=30) and use asr_marginal to reconstruct ancestral states. For the same set of trees, use ace as well, and compare the reconstructions to each other. Is it obvious that marginal reconstruction with the BiSSE model out- performs the simpler Markov models in ace? Are there different combinations of parameters where that is the case? What if you use trees with more taxa (say, n=100 or n=500?)"
  },
  {
    "objectID": "topics/sse_models/asr_bisse.html#section-1",
    "href": "topics/sse_models/asr_bisse.html#section-1",
    "title": "Ancestral state reconstructions using BiSSE",
    "section": "2.",
    "text": "2.\nIn your simulations, do you notice particular situations where reconstructions tend to be inaccurate (when does BiSSE get “tricked”)? Are there particular configurations of tip data that present challenges for the BiSSE model?"
  },
  {
    "objectID": "topics/sse_models/asr_bisse.html#section-2",
    "href": "topics/sse_models/asr_bisse.html#section-2",
    "title": "Ancestral state reconstructions using BiSSE",
    "section": "3.",
    "text": "3.\nSet diversification rates in both states equal to each other when simulating yout trees, and likewise for the extinction rates. Compare models that impose constraints to reduce the number of parameters to estimate with models that impose no constraints, and explore the accuracy of reconstructions in each case. How sensitive are reconstructions to the parametric constraints you impose?"
  },
  {
    "objectID": "topics/sse_models/asr_bisse.html#section-3",
    "href": "topics/sse_models/asr_bisse.html#section-3",
    "title": "Ancestral state reconstructions using BiSSE",
    "section": "4.",
    "text": "4.\nUse one of the example datasets of your choice, and carry out ancestral state reconstructions using BiSSE, comparing to results from either ace or simmap."
  }
]